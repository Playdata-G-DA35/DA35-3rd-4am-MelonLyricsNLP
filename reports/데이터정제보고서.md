# 🎤 데이터 정제 보고서

## 데이터 소스

### 정제할 데이터 소개
- 정제할 데이터 소스는 멜론(Melon) 음악 가사 데이터셋입니다.
- 이 데이터셋은 1960년대부터 2020년대에 발표된 음악의 가사를 포함하고 있습니다.
- 멜론 시대별 100위 가사 데이터를 가져오기 위해선 `Selenium` 을 사용해 스크롤을 해당 위치까지 이동후 `Bs4` 를 이용하여 노래의 각 고유 id 값을 추출해 내었습니다. 그 후 `async` 를 이용하여 고유 id 값을 가지고 비동기처리를 하여 작업 시간을 단축시켰습니다.

## 데이터 품질 문제점

### 데이터 문제점 식별
- 반복되는 문자가 많고 , 대부분의 가사들이 한글과 영어로 섞여있어서 나눠서 정제를 해야 합니다.
- 동사의 데이터 값은 비슷한 값이 너무 많이 나와서 중요도를 판별할 수 있을까 생각했습니다.
- 한 글자 단어 (손 , 눈 , 차) 이러한 단어들이 많이 등장해 이 값을 불용어 값으로 처리해야할까 생각했습니다.
### 정제의 필요성 
- TF-IDF란, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서군에서 중요한지를 나타내는 통계적 수치입니다.
- TF (단어빈도,Term Frequency) : 특정한 단어가 문서 내에서 얼마나 자주등장하는지 나타는 값으로 수치가 높을수록 문서내에서 중요하다고 생각을 할 수 있습니다.
- DF (문서빈도,Document Frequency) : 단어 자체가 문서군 내에서 자주 사용하는 경우에는 그 단어가 흔하게 등장한다 라고 생각을 할 수 있습니다. IDF는 문서빈도의 역수 값이며 “역문서빈도”라고 합니다.
- IDF : 문서군의 성격에 따라 결정됩니다. 예를들어 “코딩”이라는 단어는 일반적인 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심 단어가 될수 있지만 “코딩”에 대한 문서나 관련된 “소프트웨어”문서군 사이에서는 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 됩니다.
- TF-IDF : 특정 문서 내에서 단어 빈도가 높을 수록, 그리고 전체 문서들 중 그 단어를 포함한 문서가 적을 수록 TF-IDF값이 높아집니다. 따라서 이 값을 이용하면 모든 문서에 흔하게 나타나는 단어를 걸러내는 효과를 얻을 수 있게 됩니다. IDF의 로그 함수 안의 값은 항상 1 이상이므로, IDF값과 TF-IDF값은 항상 0 이상이 됩니다. 특정 단어를 포함하는 문서들이 많을 수록 로그 함수 안의 값이 1에 가까워지게 되고, 이 경우 IDF값과 TF-IDF값은 0에 가까워지게 됩니다.
## 데이터 정제 방법

### 정제 방법 작성
- 결측치 제거
    - dropna() 활용하여 결측치 제거
- Tokenize
    - 불용어 설정 : 한국어, 영어 불용어 제거하여 정제
            (1) nltk stop words "english"
            (2) 한국어 불용어 사전 사용
            (3) 추가적으로 의미 없는 불용어 개별적으로 업데이트하여 제거
    - 정규 표현식 : 특수문자 제거 및 영어, 한국어 구분
            (1) 정규 표현식을 통해 한국어/영어 구분
            (2) 특수문자 제거
    - 영어 토큰화(품사태깅, 원형 복원) : n,a 품사 태깅 및 반환 후 원형복원 진행
            (1) nltk word_tokenize 활용하여 토큰화
            (2) nltk pos_tag 활용하여 n, a 반환
            (3) WordNetLemmatizer 활용하여 원형 복원
    - 한국어 토큰화(품사태깅, 원형 복원) : n,a 품사 태깅 및 반환 후 원형복원 진행
            (1) okt 활용하여 토큰화
            (2) okt.pos 활용하여 n, a 반환
            (3) okt.pos( , stem=True)로 설정하여 원형 복원
### 각 기법이 적용된 예
- 불용어 제거
    - nltk_stopwords(’english) : 영어 불용어 제거
    - nltk_stopwords.update : 불용어 업데이트 및 추가 제거
- 정규 표현식 영어/한국어 구분
    - re.compile('[^ ㄱ-ㅣ 가-힣]') : 정규 표현식을 통해 한국어 구분하고 객체 생성
    - re.compile('[^a-zA-Z\s]') : 정규 표현식을 통해 한국어 구분하고 객체 생성
- 토큰화 (영어 - 불용어 설정, 품사 태깅(명사, 형용사 반환), 원형 복원
    - nltk 활용
    - not in stop_words : stop_words 사전에 있지 않은 단어들을 가져오게 함
    - pos_tag : 품사 태깅
    - 'n' and word.lower() , 'a' and word.lower() : 명사 or 형용사 만 가져오게 조건문 작성
    - .lemmatize(word.lower(), pos='a')), .lemmatize(word.lower(), pos='n')) :  영어 원형 복원
    - word_tokenize : 영어 토큰화 진행
- 토큰화 (한국어 - 불용어 설정, 품사 태깅(명사, 형용사 반환), 원형 복원
    - okt 활용
    - if pos == 'Noun' and word not in stop_words: 조건문 작성( 불용어 설정, 및 명사 반환)
    - okt.pos(result_kr, stem=True) :  한국어 결과로 받고 품사 태깅 및 원형 복원
## 결과 및 효과

### 정제 전과 정제 후 변화결과 요약
- 토큰화
    - 정제 전 : 1960.csv
        - 하루의 일을 끝내고 돌아가는 거리엔 사람의 물결 하늘엔 별이 하나 둘 반짝이면 가로등 하나 둘 꽃 피네 허공을 스치는 바람은 차고 흐뭇한 마음은 애드베룬 가벼운 발길 헤어질 때 인사는 내일 또 다시 만납시다 하루의 일을 끝내고 돌아가는 거리엔 사람의 물결 하늘엔 별이 하나 둘 반짝이면 가로등 하나 둘 꽃 피네 허공을 스치는 바람은 차고 흐뭇한 마음은 애드베룬 가벼운 발길 헤어질 때 인사는 내일 또 다시 만납시다 내일 또 다시 만납시다
    - 정제 후 - 토큰화하여 추출
        - 하루, 거리, 사람, 물결, 하늘, 별, 가로등, 꽃, 피, 허공, 바람, 차고, 마음, 애드베룬, 발길, 인사, 내일, 다시
## 향후 계획

머신러닝 및 자연어 처리 기술을 활용하여 감정 분석을 시도할 계획입니다. 각 연대별로 가사의 감정 변화 추이를 추적하고, 이를 통해 시대별로 어떤 감정을 표현하고 있는지 분석할 예정입니다. 나아가 기존의 시대별 트렌드 분석과 결합하여 음악이 시대적, 문화적 변화에 미치는 영향을 확인하고자 합니다. 